{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from everything import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_num=100\n",
    "aggdf = pd.read_csv(f\"aggregated_{agg_num}_std.csv\", index_col =False)\n",
    "dat= aggdf[[\"acoustic_data\", \"time_to_failure\"]].to_numpy()\n",
    "# oaggdf = pd.read_csv(f\"aggregated_{agg_num}_mean.csv\", index_col =False)\n",
    "# odata = oaggdf[[\"acoustic_data\"]].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.hstack([odata,dat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3356, 1500, 1), (3356, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#maybe normalize about zero?\n",
    "np.random.seed(42)\n",
    "ratio = 0.2\n",
    "num_channels = 1\n",
    "n = len(data)\n",
    "sample_size = int(150_000/agg_num)\n",
    "split_data = np.array(np.split(data[:(n//sample_size)*sample_size], n//sample_size))\n",
    "# labels are first time to appear \n",
    "split_inputs = split_data[:,:,0].reshape(-1, sample_size,1)\n",
    "split_labels = split_data[:,0,1].reshape(-1,1)\n",
    "#split_inputs_fft = np.fft.hfft(split_inputs, axis=1)[:,:1500,:]\n",
    "#split_inputs = np.concatenate([split_inputs_fft,split_inputs], axis=2)\n",
    "p = np.random.permutation(len(split_inputs))\n",
    "split_inputs, split_labels = split_inputs[p], split_labels[p]\n",
    "train_num = int(len(split_inputs)*ratio)\n",
    "train_X, test_X = split_inputs[train_num:], split_inputs[:train_num]\n",
    "train_y, test_y = split_labels[train_num:], split_labels[:train_num]\n",
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 1491, 10)          110       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 497, 10)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 488, 20)           2020      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 162, 20)           0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 162, 20)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 153, 40)           8040      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 10,211\n",
      "Trainable params: 10,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, (None, 1500, 1), (None, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Reshape, MaxPooling1D, GlobalAveragePooling1D, Dropout\n",
    "model = Sequential()\n",
    "model.add(Conv1D(10, 10, activation='relu', input_shape=(sample_size, num_channels)))\n",
    "#model.add(Conv1D(10, 10, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(20, 10, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(40, 10, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(10, activation='linear'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary(), model.input_shape, model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3356 samples, validate on 838 samples\n",
      "Epoch 1/200\n",
      "3356/3356 [==============================] - 2s 641us/step - loss: 3.7193 - val_loss: 2.4750\n",
      "Epoch 2/200\n",
      "3356/3356 [==============================] - 0s 146us/step - loss: 2.6597 - val_loss: 2.4784\n",
      "Epoch 3/200\n",
      "3356/3356 [==============================] - 1s 280us/step - loss: 2.5190 - val_loss: 2.3095\n",
      "Epoch 4/200\n",
      "3356/3356 [==============================] - 1s 180us/step - loss: 2.5286 - val_loss: 2.3563\n",
      "Epoch 5/200\n",
      "3356/3356 [==============================] - 1s 153us/step - loss: 2.5038 - val_loss: 2.3080\n",
      "Epoch 6/200\n",
      "3356/3356 [==============================] - 1s 165us/step - loss: 2.4833 - val_loss: 2.3393\n",
      "Epoch 7/200\n",
      "3356/3356 [==============================] - 1s 186us/step - loss: 2.4749 - val_loss: 2.4869\n",
      "Epoch 8/200\n",
      "3356/3356 [==============================] - 1s 150us/step - loss: 2.4173 - val_loss: 2.5632\n",
      "Epoch 9/200\n",
      "3356/3356 [==============================] - 1s 150us/step - loss: 2.4053 - val_loss: 2.2683\n",
      "Epoch 10/200\n",
      "3356/3356 [==============================] - 1s 151us/step - loss: 2.4221 - val_loss: 2.2390\n",
      "Epoch 11/200\n",
      "3356/3356 [==============================] - 1s 157us/step - loss: 2.3897 - val_loss: 2.2258\n",
      "Epoch 12/200\n",
      "3356/3356 [==============================] - 1s 161us/step - loss: 2.3570 - val_loss: 2.2298\n",
      "Epoch 13/200\n",
      "3356/3356 [==============================] - 1s 155us/step - loss: 2.3803 - val_loss: 2.2237\n",
      "Epoch 14/200\n",
      "3356/3356 [==============================] - 1s 181us/step - loss: 2.3688 - val_loss: 2.2050\n",
      "Epoch 15/200\n",
      "3356/3356 [==============================] - 1s 190us/step - loss: 2.4090 - val_loss: 2.2162\n",
      "Epoch 16/200\n",
      "3356/3356 [==============================] - 1s 183us/step - loss: 2.3653 - val_loss: 2.2147\n",
      "Epoch 17/200\n",
      "3356/3356 [==============================] - 1s 152us/step - loss: 2.3598 - val_loss: 2.2825\n",
      "Epoch 18/200\n",
      "3356/3356 [==============================] - 1s 161us/step - loss: 2.4136 - val_loss: 2.2396\n",
      "Epoch 19/200\n",
      "3356/3356 [==============================] - 1s 151us/step - loss: 2.4023 - val_loss: 2.2118\n",
      "Epoch 20/200\n",
      "3356/3356 [==============================] - 1s 183us/step - loss: 2.3412 - val_loss: 2.2152\n",
      "Epoch 21/200\n",
      "3356/3356 [==============================] - 1s 189us/step - loss: 2.3938 - val_loss: 2.2538\n",
      "Epoch 22/200\n",
      "3356/3356 [==============================] - 1s 229us/step - loss: 2.3779 - val_loss: 2.2049\n",
      "Epoch 23/200\n",
      "3356/3356 [==============================] - 1s 218us/step - loss: 2.3490 - val_loss: 2.2511\n",
      "Epoch 24/200\n",
      "3356/3356 [==============================] - 1s 202us/step - loss: 2.2983 - val_loss: 2.2585\n",
      "Epoch 25/200\n",
      "3356/3356 [==============================] - 1s 205us/step - loss: 2.3571 - val_loss: 2.2061\n",
      "Epoch 26/200\n",
      "3356/3356 [==============================] - 1s 210us/step - loss: 2.3043 - val_loss: 2.2003\n",
      "Epoch 27/200\n",
      "3356/3356 [==============================] - 1s 186us/step - loss: 2.3210 - val_loss: 2.2547\n",
      "Epoch 28/200\n",
      "3356/3356 [==============================] - 1s 189us/step - loss: 2.3242 - val_loss: 2.2009\n",
      "Epoch 29/200\n",
      "3356/3356 [==============================] - 1s 191us/step - loss: 2.3300 - val_loss: 2.1989\n",
      "Epoch 30/200\n",
      "3356/3356 [==============================] - 1s 166us/step - loss: 2.3620 - val_loss: 2.2048\n",
      "Epoch 31/200\n",
      "3356/3356 [==============================] - 1s 220us/step - loss: 2.3414 - val_loss: 2.2110\n",
      "Epoch 32/200\n",
      "3356/3356 [==============================] - 1s 182us/step - loss: 2.2837 - val_loss: 2.2074\n",
      "Epoch 33/200\n",
      "3356/3356 [==============================] - 1s 185us/step - loss: 2.3228 - val_loss: 2.1985\n",
      "Epoch 34/200\n",
      "3356/3356 [==============================] - 1s 185us/step - loss: 2.3261 - val_loss: 2.2029\n",
      "Epoch 35/200\n",
      "3356/3356 [==============================] - 1s 176us/step - loss: 2.3072 - val_loss: 2.1892\n",
      "Epoch 36/200\n",
      "3356/3356 [==============================] - 1s 186us/step - loss: 2.3355 - val_loss: 2.2052\n",
      "Epoch 37/200\n",
      "3356/3356 [==============================] - 1s 168us/step - loss: 2.3250 - val_loss: 2.1944\n",
      "Epoch 38/200\n",
      "3356/3356 [==============================] - 1s 174us/step - loss: 2.3132 - val_loss: 2.1905\n",
      "Epoch 39/200\n",
      "3356/3356 [==============================] - 1s 173us/step - loss: 2.3204 - val_loss: 2.2147\n",
      "Epoch 40/200\n",
      "3356/3356 [==============================] - 1s 188us/step - loss: 2.3356 - val_loss: 2.3912\n",
      "Epoch 41/200\n",
      "3356/3356 [==============================] - 1s 215us/step - loss: 2.3032 - val_loss: 2.1922\n",
      "Epoch 42/200\n",
      "3356/3356 [==============================] - 1s 181us/step - loss: 2.3306 - val_loss: 2.2158\n",
      "Epoch 43/200\n",
      "3356/3356 [==============================] - 1s 204us/step - loss: 2.2633 - val_loss: 2.1862\n",
      "Epoch 44/200\n",
      "3356/3356 [==============================] - 1s 220us/step - loss: 2.3161 - val_loss: 2.1922\n",
      "Epoch 45/200\n",
      "3356/3356 [==============================] - 1s 212us/step - loss: 2.3130 - val_loss: 2.2341\n",
      "Epoch 46/200\n",
      "3356/3356 [==============================] - 1s 204us/step - loss: 2.2962 - val_loss: 2.2137\n",
      "Epoch 47/200\n",
      "3356/3356 [==============================] - 1s 228us/step - loss: 2.3131 - val_loss: 2.2644\n",
      "Epoch 48/200\n",
      "3356/3356 [==============================] - 1s 182us/step - loss: 2.3027 - val_loss: 2.1979\n",
      "Epoch 49/200\n",
      "3356/3356 [==============================] - 1s 219us/step - loss: 2.2829 - val_loss: 2.1962\n",
      "Epoch 50/200\n",
      "3356/3356 [==============================] - 1s 206us/step - loss: 2.2833 - val_loss: 2.2168\n",
      "Epoch 51/200\n",
      "3356/3356 [==============================] - 1s 282us/step - loss: 2.2809 - val_loss: 2.1892\n",
      "Epoch 52/200\n",
      "3356/3356 [==============================] - 1s 267us/step - loss: 2.2849 - val_loss: 2.2135\n",
      "Epoch 53/200\n",
      "3356/3356 [==============================] - 1s 271us/step - loss: 2.2520 - val_loss: 2.1905\n",
      "Epoch 54/200\n",
      "3356/3356 [==============================] - 1s 209us/step - loss: 2.2973 - val_loss: 2.1880\n",
      "Epoch 55/200\n",
      "3356/3356 [==============================] - 1s 199us/step - loss: 2.2783 - val_loss: 2.2084\n",
      "Epoch 56/200\n",
      "3356/3356 [==============================] - 1s 217us/step - loss: 2.2791 - val_loss: 2.2851\n",
      "Epoch 57/200\n",
      "3356/3356 [==============================] - 1s 264us/step - loss: 2.2461 - val_loss: 2.1856\n",
      "Epoch 58/200\n",
      "3356/3356 [==============================] - 1s 213us/step - loss: 2.2833 - val_loss: 2.1998\n",
      "Epoch 59/200\n",
      "3356/3356 [==============================] - 1s 222us/step - loss: 2.2692 - val_loss: 2.2092\n",
      "Epoch 60/200\n",
      "3356/3356 [==============================] - 1s 211us/step - loss: 2.2675 - val_loss: 2.2688\n",
      "Epoch 61/200\n",
      "3356/3356 [==============================] - 1s 202us/step - loss: 2.2603 - val_loss: 2.1970\n",
      "Epoch 62/200\n",
      "3356/3356 [==============================] - 1s 206us/step - loss: 2.2595 - val_loss: 2.1921\n",
      "Epoch 63/200\n",
      "3356/3356 [==============================] - 1s 209us/step - loss: 2.2718 - val_loss: 2.2232\n",
      "Epoch 64/200\n",
      "3356/3356 [==============================] - 1s 215us/step - loss: 2.2759 - val_loss: 2.2056\n",
      "Epoch 65/200\n",
      "3356/3356 [==============================] - 1s 185us/step - loss: 2.2364 - val_loss: 2.1838\n",
      "Epoch 66/200\n",
      "3356/3356 [==============================] - 1s 179us/step - loss: 2.2373 - val_loss: 2.1766\n",
      "Epoch 67/200\n",
      "3356/3356 [==============================] - 1s 188us/step - loss: 2.2602 - val_loss: 2.1893\n",
      "Epoch 68/200\n",
      "3356/3356 [==============================] - 1s 233us/step - loss: 2.2498 - val_loss: 2.1921\n",
      "Epoch 69/200\n",
      "3356/3356 [==============================] - 1s 280us/step - loss: 2.2590 - val_loss: 2.1837\n",
      "Epoch 70/200\n",
      "3356/3356 [==============================] - 1s 200us/step - loss: 2.2473 - val_loss: 2.2060\n",
      "Epoch 71/200\n",
      "3356/3356 [==============================] - 1s 228us/step - loss: 2.2387 - val_loss: 2.1789\n",
      "Epoch 72/200\n",
      "3356/3356 [==============================] - 1s 199us/step - loss: 2.2452 - val_loss: 2.1952\n",
      "Epoch 73/200\n",
      "3356/3356 [==============================] - 1s 167us/step - loss: 2.2500 - val_loss: 2.2074\n",
      "Epoch 74/200\n",
      "3356/3356 [==============================] - 1s 179us/step - loss: 2.2650 - val_loss: 2.1907\n",
      "Epoch 75/200\n",
      "3356/3356 [==============================] - 1s 214us/step - loss: 2.2475 - val_loss: 2.1873\n",
      "Epoch 76/200\n",
      "3356/3356 [==============================] - 1s 303us/step - loss: 2.2546 - val_loss: 2.1723\n",
      "Epoch 77/200\n",
      "3356/3356 [==============================] - 1s 201us/step - loss: 2.2290 - val_loss: 2.1831\n",
      "Epoch 78/200\n",
      "3356/3356 [==============================] - 1s 273us/step - loss: 2.2383 - val_loss: 2.1792\n",
      "Epoch 79/200\n",
      "3356/3356 [==============================] - 1s 310us/step - loss: 2.2813 - val_loss: 2.1659\n",
      "Epoch 80/200\n",
      "3356/3356 [==============================] - 1s 252us/step - loss: 2.2625 - val_loss: 2.1801\n",
      "Epoch 81/200\n",
      "3356/3356 [==============================] - 1s 279us/step - loss: 2.2189 - val_loss: 2.1856\n",
      "Epoch 82/200\n",
      "3356/3356 [==============================] - 1s 265us/step - loss: 2.2122 - val_loss: 2.1911\n",
      "Epoch 83/200\n",
      "3356/3356 [==============================] - 1s 219us/step - loss: 2.2418 - val_loss: 2.2256\n",
      "Epoch 84/200\n",
      "3356/3356 [==============================] - 1s 195us/step - loss: 2.2116 - val_loss: 2.1872\n",
      "Epoch 85/200\n",
      "3356/3356 [==============================] - 1s 212us/step - loss: 2.2234 - val_loss: 2.2469\n",
      "Epoch 86/200\n",
      "3356/3356 [==============================] - 1s 205us/step - loss: 2.2025 - val_loss: 2.1760\n",
      "Epoch 87/200\n",
      "3356/3356 [==============================] - 1s 180us/step - loss: 2.2213 - val_loss: 2.1725\n",
      "Epoch 88/200\n",
      "3356/3356 [==============================] - 1s 185us/step - loss: 2.2319 - val_loss: 2.1805\n",
      "Epoch 89/200\n",
      "3356/3356 [==============================] - 1s 173us/step - loss: 2.2154 - val_loss: 2.1769\n",
      "Epoch 90/200\n",
      "3356/3356 [==============================] - 1s 177us/step - loss: 2.2177 - val_loss: 2.1875\n",
      "Epoch 91/200\n",
      "3356/3356 [==============================] - 1s 179us/step - loss: 2.2446 - val_loss: 2.1681\n",
      "Epoch 92/200\n",
      "3356/3356 [==============================] - 1s 221us/step - loss: 2.2437 - val_loss: 2.1819\n",
      "Epoch 93/200\n",
      "3356/3356 [==============================] - 1s 216us/step - loss: 2.2519 - val_loss: 2.1599\n",
      "Epoch 94/200\n",
      "3356/3356 [==============================] - 1s 202us/step - loss: 2.2286 - val_loss: 2.1527\n",
      "Epoch 95/200\n",
      "3356/3356 [==============================] - 1s 271us/step - loss: 2.2182 - val_loss: 2.1878\n",
      "Epoch 96/200\n",
      "3356/3356 [==============================] - 1s 257us/step - loss: 2.2515 - val_loss: 2.1736\n",
      "Epoch 97/200\n",
      "3356/3356 [==============================] - 1s 266us/step - loss: 2.2296 - val_loss: 2.1989\n",
      "Epoch 98/200\n",
      "3356/3356 [==============================] - 1s 261us/step - loss: 2.2254 - val_loss: 2.1995\n",
      "Epoch 99/200\n",
      "3356/3356 [==============================] - 1s 266us/step - loss: 2.2314 - val_loss: 2.1670\n",
      "Epoch 100/200\n",
      "3356/3356 [==============================] - 1s 234us/step - loss: 2.1899 - val_loss: 2.2053\n",
      "Epoch 101/200\n",
      "3356/3356 [==============================] - 1s 232us/step - loss: 2.2227 - val_loss: 2.1853\n",
      "Epoch 102/200\n",
      "3356/3356 [==============================] - 1s 327us/step - loss: 2.2216 - val_loss: 2.1756\n",
      "Epoch 103/200\n",
      "3356/3356 [==============================] - 1s 270us/step - loss: 2.1910 - val_loss: 2.2153\n",
      "Epoch 104/200\n",
      "3356/3356 [==============================] - 1s 198us/step - loss: 2.2895 - val_loss: 2.2008\n",
      "Epoch 105/200\n",
      "3356/3356 [==============================] - 1s 172us/step - loss: 2.1806 - val_loss: 2.1927\n",
      "Epoch 106/200\n",
      "3356/3356 [==============================] - 1s 183us/step - loss: 2.2060 - val_loss: 2.1734\n",
      "Epoch 107/200\n",
      "3356/3356 [==============================] - 1s 172us/step - loss: 2.2202 - val_loss: 2.1937\n",
      "Epoch 108/200\n",
      "3356/3356 [==============================] - 1s 199us/step - loss: 2.2049 - val_loss: 2.1777\n",
      "Epoch 109/200\n",
      "3356/3356 [==============================] - 1s 224us/step - loss: 2.2094 - val_loss: 2.1779\n",
      "Epoch 110/200\n",
      "3356/3356 [==============================] - 1s 253us/step - loss: 2.2185 - val_loss: 2.1758\n",
      "Epoch 111/200\n",
      "3356/3356 [==============================] - 1s 217us/step - loss: 2.2451 - val_loss: 2.1811\n",
      "Epoch 112/200\n",
      "3356/3356 [==============================] - 1s 255us/step - loss: 2.2121 - val_loss: 2.1933\n",
      "Epoch 113/200\n",
      "3356/3356 [==============================] - 1s 255us/step - loss: 2.2215 - val_loss: 2.1704\n",
      "Epoch 114/200\n",
      "3356/3356 [==============================] - 1s 246us/step - loss: 2.1893 - val_loss: 2.1741\n",
      "Epoch 115/200\n",
      "3356/3356 [==============================] - 1s 213us/step - loss: 2.2057 - val_loss: 2.1932\n",
      "Epoch 116/200\n",
      "3356/3356 [==============================] - 1s 219us/step - loss: 2.2041 - val_loss: 2.1892\n",
      "Epoch 117/200\n",
      "3356/3356 [==============================] - 1s 217us/step - loss: 2.2022 - val_loss: 2.3230\n",
      "Epoch 118/200\n",
      "3356/3356 [==============================] - 1s 210us/step - loss: 2.2318 - val_loss: 2.2543\n",
      "Epoch 119/200\n",
      "3356/3356 [==============================] - 1s 220us/step - loss: 2.2214 - val_loss: 2.1882\n",
      "Epoch 120/200\n",
      "3356/3356 [==============================] - 1s 204us/step - loss: 2.1922 - val_loss: 2.1685\n",
      "Epoch 121/200\n",
      "3356/3356 [==============================] - 1s 193us/step - loss: 2.2185 - val_loss: 2.1800\n",
      "Epoch 122/200\n",
      "3356/3356 [==============================] - 1s 213us/step - loss: 2.2135 - val_loss: 2.1845\n",
      "Epoch 123/200\n",
      "3356/3356 [==============================] - 1s 207us/step - loss: 2.1911 - val_loss: 2.2060\n",
      "Epoch 124/200\n",
      "3356/3356 [==============================] - 1s 220us/step - loss: 2.1750 - val_loss: 2.1800\n",
      "Epoch 125/200\n",
      "3356/3356 [==============================] - 1s 196us/step - loss: 2.1847 - val_loss: 2.1997\n",
      "Epoch 126/200\n",
      "3356/3356 [==============================] - 1s 207us/step - loss: 2.2032 - val_loss: 2.1587\n",
      "Epoch 127/200\n",
      "3356/3356 [==============================] - 1s 202us/step - loss: 2.1929 - val_loss: 2.1763\n",
      "Epoch 128/200\n",
      "3356/3356 [==============================] - 1s 185us/step - loss: 2.1789 - val_loss: 2.1648\n",
      "Epoch 129/200\n",
      "3356/3356 [==============================] - 1s 229us/step - loss: 2.1912 - val_loss: 2.1796\n",
      "Epoch 130/200\n",
      "3356/3356 [==============================] - 1s 226us/step - loss: 2.1799 - val_loss: 2.2095\n",
      "Epoch 131/200\n",
      "3356/3356 [==============================] - 1s 196us/step - loss: 2.2174 - val_loss: 2.2138\n",
      "Epoch 132/200\n",
      "3356/3356 [==============================] - 1s 185us/step - loss: 2.1922 - val_loss: 2.1886\n",
      "Epoch 133/200\n",
      "3356/3356 [==============================] - 1s 205us/step - loss: 2.2018 - val_loss: 2.3695\n",
      "Epoch 134/200\n",
      "3356/3356 [==============================] - 1s 241us/step - loss: 2.1749 - val_loss: 2.1749\n",
      "Epoch 135/200\n",
      "3356/3356 [==============================] - 1s 225us/step - loss: 2.1890 - val_loss: 2.1939\n",
      "Epoch 136/200\n",
      "3356/3356 [==============================] - 1s 250us/step - loss: 2.1855 - val_loss: 2.2035\n",
      "Epoch 137/200\n",
      "3356/3356 [==============================] - 1s 237us/step - loss: 2.1861 - val_loss: 2.1833\n",
      "Epoch 138/200\n",
      "3356/3356 [==============================] - 1s 222us/step - loss: 2.2082 - val_loss: 2.2099\n",
      "Epoch 139/200\n",
      "3356/3356 [==============================] - 1s 213us/step - loss: 2.2026 - val_loss: 2.2674\n",
      "Epoch 140/200\n",
      "3356/3356 [==============================] - 1s 212us/step - loss: 2.1899 - val_loss: 2.1904\n",
      "Epoch 141/200\n",
      "3356/3356 [==============================] - 1s 205us/step - loss: 2.1840 - val_loss: 2.1764\n",
      "Epoch 142/200\n",
      "3356/3356 [==============================] - 1s 290us/step - loss: 2.1755 - val_loss: 2.1930\n",
      "Epoch 143/200\n",
      "3356/3356 [==============================] - 1s 197us/step - loss: 2.1880 - val_loss: 2.1947\n",
      "Epoch 144/200\n",
      "3356/3356 [==============================] - 1s 198us/step - loss: 2.1650 - val_loss: 2.2015\n",
      "Epoch 145/200\n",
      "3356/3356 [==============================] - 1s 190us/step - loss: 2.1542 - val_loss: 2.1791\n",
      "Epoch 146/200\n",
      "3356/3356 [==============================] - 1s 181us/step - loss: 2.1921 - val_loss: 2.1916\n",
      "Epoch 147/200\n",
      "3356/3356 [==============================] - 1s 182us/step - loss: 2.1806 - val_loss: 2.2088\n",
      "Epoch 148/200\n",
      "3356/3356 [==============================] - 1s 190us/step - loss: 2.1911 - val_loss: 2.1892\n",
      "Epoch 149/200\n",
      "3356/3356 [==============================] - 1s 207us/step - loss: 2.1708 - val_loss: 2.1685\n",
      "Epoch 150/200\n",
      "3356/3356 [==============================] - 1s 197us/step - loss: 2.1456 - val_loss: 2.1820\n",
      "Epoch 151/200\n",
      "3356/3356 [==============================] - 1s 181us/step - loss: 2.1832 - val_loss: 2.1886\n",
      "Epoch 152/200\n",
      "3356/3356 [==============================] - 1s 182us/step - loss: 2.1723 - val_loss: 2.1789\n",
      "Epoch 153/200\n",
      "3356/3356 [==============================] - 1s 186us/step - loss: 2.1787 - val_loss: 2.1888\n",
      "Epoch 154/200\n",
      "3356/3356 [==============================] - 1s 177us/step - loss: 2.1576 - val_loss: 2.1683\n",
      "Epoch 155/200\n",
      "3356/3356 [==============================] - 1s 177us/step - loss: 2.1767 - val_loss: 2.2083\n",
      "Epoch 156/200\n",
      "3356/3356 [==============================] - 1s 186us/step - loss: 2.1552 - val_loss: 2.2235\n",
      "Epoch 157/200\n",
      "3356/3356 [==============================] - 1s 199us/step - loss: 2.1657 - val_loss: 2.2079\n",
      "Epoch 158/200\n",
      "3356/3356 [==============================] - 1s 203us/step - loss: 2.2038 - val_loss: 2.1990\n",
      "Epoch 159/200\n",
      "3356/3356 [==============================] - 1s 182us/step - loss: 2.1946 - val_loss: 2.2529\n",
      "Epoch 160/200\n",
      "3356/3356 [==============================] - 1s 209us/step - loss: 2.1702 - val_loss: 2.1931\n",
      "Epoch 161/200\n",
      "3356/3356 [==============================] - 1s 221us/step - loss: 2.1833 - val_loss: 2.1864\n",
      "Epoch 162/200\n",
      "3356/3356 [==============================] - 1s 216us/step - loss: 2.1718 - val_loss: 2.1972\n",
      "Epoch 163/200\n",
      "3356/3356 [==============================] - 1s 246us/step - loss: 2.1737 - val_loss: 2.2015\n",
      "Epoch 164/200\n",
      "3356/3356 [==============================] - 1s 212us/step - loss: 2.1735 - val_loss: 2.1912\n",
      "Epoch 165/200\n",
      "3356/3356 [==============================] - 1s 211us/step - loss: 2.1594 - val_loss: 2.1902\n",
      "Epoch 166/200\n",
      "3356/3356 [==============================] - 1s 238us/step - loss: 2.1647 - val_loss: 2.2111\n",
      "Epoch 167/200\n",
      "3356/3356 [==============================] - 1s 242us/step - loss: 2.1772 - val_loss: 2.2271\n",
      "Epoch 168/200\n",
      "3356/3356 [==============================] - 1s 205us/step - loss: 2.1737 - val_loss: 2.1784\n",
      "Epoch 169/200\n",
      "3356/3356 [==============================] - 1s 197us/step - loss: 2.1782 - val_loss: 2.2017\n",
      "Epoch 170/200\n",
      "3356/3356 [==============================] - 1s 216us/step - loss: 2.1731 - val_loss: 2.1619\n",
      "Epoch 171/200\n",
      "3356/3356 [==============================] - 1s 196us/step - loss: 2.1852 - val_loss: 2.1809\n",
      "Epoch 172/200\n",
      "3356/3356 [==============================] - 1s 218us/step - loss: 2.1614 - val_loss: 2.1647\n",
      "Epoch 173/200\n",
      "3356/3356 [==============================] - 1s 209us/step - loss: 2.1586 - val_loss: 2.1836\n",
      "Epoch 174/200\n",
      "3356/3356 [==============================] - 1s 183us/step - loss: 2.1486 - val_loss: 2.1856\n",
      "Epoch 175/200\n",
      "3356/3356 [==============================] - 1s 217us/step - loss: 2.1590 - val_loss: 2.2024\n",
      "Epoch 176/200\n",
      "3356/3356 [==============================] - 1s 205us/step - loss: 2.1549 - val_loss: 2.1748\n",
      "Epoch 177/200\n",
      "3356/3356 [==============================] - 1s 192us/step - loss: 2.1394 - val_loss: 2.1918\n",
      "Epoch 178/200\n",
      "3356/3356 [==============================] - 1s 189us/step - loss: 2.1533 - val_loss: 2.2076\n",
      "Epoch 179/200\n",
      "3356/3356 [==============================] - 1s 199us/step - loss: 2.1548 - val_loss: 2.2153\n",
      "Epoch 180/200\n",
      "3356/3356 [==============================] - 1s 209us/step - loss: 2.1524 - val_loss: 2.1541\n",
      "Epoch 181/200\n",
      "3356/3356 [==============================] - 1s 278us/step - loss: 2.1565 - val_loss: 2.2044\n",
      "Epoch 182/200\n",
      "3356/3356 [==============================] - 1s 244us/step - loss: 2.1394 - val_loss: 2.1784\n",
      "Epoch 183/200\n",
      "3356/3356 [==============================] - 1s 224us/step - loss: 2.1373 - val_loss: 2.2083\n",
      "Epoch 184/200\n",
      "3356/3356 [==============================] - 1s 220us/step - loss: 2.1502 - val_loss: 2.1589\n",
      "Epoch 185/200\n",
      "3356/3356 [==============================] - 1s 223us/step - loss: 2.1441 - val_loss: 2.1728\n",
      "Epoch 186/200\n",
      "3356/3356 [==============================] - 1s 209us/step - loss: 2.1555 - val_loss: 2.2003\n",
      "Epoch 187/200\n",
      "3356/3356 [==============================] - 1s 199us/step - loss: 2.1561 - val_loss: 2.1491\n",
      "Epoch 188/200\n",
      "3356/3356 [==============================] - 1s 187us/step - loss: 2.1565 - val_loss: 2.1780\n",
      "Epoch 189/200\n",
      "3356/3356 [==============================] - 1s 219us/step - loss: 2.1549 - val_loss: 2.1551\n",
      "Epoch 190/200\n",
      "3356/3356 [==============================] - 1s 223us/step - loss: 2.1556 - val_loss: 2.2085\n",
      "Epoch 191/200\n",
      "3356/3356 [==============================] - 1s 188us/step - loss: 2.1474 - val_loss: 2.2470\n",
      "Epoch 192/200\n",
      "3356/3356 [==============================] - 1s 183us/step - loss: 2.1347 - val_loss: 2.1832\n",
      "Epoch 193/200\n",
      "3356/3356 [==============================] - 1s 177us/step - loss: 2.1607 - val_loss: 2.2238\n",
      "Epoch 194/200\n",
      "3356/3356 [==============================] - 1s 188us/step - loss: 2.1420 - val_loss: 2.1953\n",
      "Epoch 195/200\n",
      "3356/3356 [==============================] - 1s 195us/step - loss: 2.1701 - val_loss: 2.2042\n",
      "Epoch 196/200\n",
      "3356/3356 [==============================] - 1s 197us/step - loss: 2.1699 - val_loss: 2.1901\n",
      "Epoch 197/200\n",
      "3356/3356 [==============================] - 1s 185us/step - loss: 2.1415 - val_loss: 2.2243\n",
      "Epoch 198/200\n",
      "3356/3356 [==============================] - 1s 192us/step - loss: 2.1525 - val_loss: 2.1921\n",
      "Epoch 199/200\n",
      "3356/3356 [==============================] - 1s 174us/step - loss: 2.1364 - val_loss: 2.1810\n",
      "Epoch 200/200\n",
      "3356/3356 [==============================] - 1s 182us/step - loss: 2.1603 - val_loss: 2.1994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x162afa4b940>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time.time()))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "model.fit(train_X, train_y, epochs=200, callbacks=[tensorboard], verbose=1, validation_data=(test_X, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.01"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(i[0] for i in test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.47841184, 2.44889173],\n",
       "       [2.51130776, 2.82505028],\n",
       "       [2.64193073, 2.71464416],\n",
       "       ...,\n",
       "       [3.64221877, 3.31960811],\n",
       "       [2.96314057, 2.79349895],\n",
       "       [2.74572211, 2.35004836]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.01],\n",
       "       [5.07],\n",
       "       [5.1 ],\n",
       "       ...,\n",
       "       [4.76],\n",
       "       [4.07],\n",
       "       [5.16]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_labels[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
